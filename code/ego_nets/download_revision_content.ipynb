{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaanguraysirin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "month_dict = { \"Ocak\":1, \"Şubat\":2, \"Mart\":3, \"Nisan\":4, \"Mayıs\":5, \"Haziran\":6, \"Temmuz\":7, \"Ağustos\":8, \"Eylül\":9, \"Ekim\":10, \"Kasım\":11, \"Aralık\":12}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_PATH= \"list.txt\"\n",
    "DATA_PATH = \"data_1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    date= date.split(\",\")[1].strip()\n",
    "    #months are in Turkish\n",
    "    for key in month_dict:\n",
    "        date = date.replace(key, str(month_dict[key]))\n",
    "    date = date.split(\" \")\n",
    "    date = date[2]+\"-\"+date[1]+\"-\"+date[0]\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return date\n",
    "\n",
    "def convert_date2(date):\n",
    "    date= date.split(\",\")[1].strip()\n",
    "    #months are in Turkish\n",
    "    for key in month_dict:\n",
    "        date = date.replace(key, str(month_dict[key]))\n",
    "    date = date.split(\" \")\n",
    "    date = date[0]+\".\"+date[1]+\".\"+date[2]\n",
    "    return str(date)\n",
    "\n",
    "\n",
    "def get_article_id(query, date_list):\n",
    "    url=\"https://tr.wikipedia.org/w/index.php?title={}&action=history&limit=5000\".format(query.replace(\" \", \"_\"))\n",
    "    resp=requests.get(url)\n",
    "    soup = bs(resp.text, \"html.parser\")\n",
    "    articles= soup.find_all(\"ul\", {\"class\":\"mw-contributions-list\"})\n",
    "    datelist= date_list.copy()\n",
    "\n",
    "    ret_map = defaultdict(list)\n",
    "\n",
    "    while True:\n",
    "        for art in articles:    \n",
    "            #if a date is smaller than the given date, return the id\n",
    "            #since the articles are sorted returned value will be the closest date to the given date \n",
    "            if datelist[0] >= convert_date(art.find(\"a\", {\"class\":\"mw-changeslist-date\"}).text):\n",
    "                #print(\"Date is:\",convert_date(art.find(\"a\", {\"class\":\"mw-changeslist-date\"}).text))\n",
    "                ret_map[query].append([art.find(\"li\")[\"data-mw-revid\"], convert_date2(art.find(\"a\", {\"class\":\"mw-changeslist-date\"}).text)])\n",
    "                datelist.pop(0)\n",
    "\n",
    "                if len(datelist)==0:\n",
    "                    return ret_map\n",
    "\n",
    "        try: #if the date is not found in the history, check the next page\n",
    "            next_page = soup.find(\"a\", {\"class\":\"mw-nextlink\"})\n",
    "            next_page = next_page[\"href\"]\n",
    "            resp=requests.get(\"https://tr.wikipedia.org\"+next_page)\n",
    "            soup = bs(resp.text, \"html.parser\")\n",
    "            articles= soup.find_all(\"ul\", {\"class\":\"mw-contributions-list\"})\n",
    "        except: #if there is no more pages to check, return None\n",
    "            print(f\"No data for below {str(datelist[0].date()) } for {query}\")\n",
    "            break\n",
    "        \n",
    "    return ret_map\n",
    "\n",
    "def get_wiki_page(pid): \n",
    "    # get the article with the given id\n",
    "    url=\"https://tr.wikipedia.org/w/index.php?oldid={}\".format(pid)\n",
    "    resp=requests.get(url)\n",
    "    soup = bs(resp.text, \"html.parser\")\n",
    "    article = soup.find(\"div\", {\"id\":\"mw-content-text\"})\n",
    "    return article.text\n",
    "\n",
    "\n",
    "def get_history_data(query, date):\n",
    "    pid = get_article_id(query, date)\n",
    "    return pid\n",
    "    article = get_wiki_page(pid)\n",
    "    return article\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage Query; Date year, month, day\n",
    "date_list= [datetime(i,1,1) for i in range(2025,2015,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for below 2019-01-01 for Ekrem İmamoğlu\n"
     ]
    }
   ],
   "source": [
    "my_article= get_history_data(\"Ekrem İmamoğlu\", date_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(my_article[\"Ekrem İmamoğlu\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LIST_PATH):\n",
    "    with open(LIST_PATH, \"w\") as f:\n",
    "        f.write(\"\\t\".join([\"ID\", \"Query\", \"Date\"])+ \"\\n\")\n",
    "\n",
    "else:\n",
    "    data= pd.read_csv(LIST_PATH, sep=\"\\t\")\n",
    "    if data.shape[0]==0:\n",
    "        id_list=[]\n",
    "    else:\n",
    "        id_list= data[\"ID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LIST_PATH, \"a\") as f:\n",
    "    for key in my_article:\n",
    "        for val in my_article[key]:\n",
    "            if int(val[0]) not in id_list:\n",
    "                f.write(\"\\t\".join([val[0], key.replace(\" \",\"_\") , val[1]])+ \"\\n\")\n",
    "                id_list.append(int(val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LIST_PATH, sep=\"\\t\")\n",
    "data = {}\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    with open(DATA_PATH, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    if os.path.getsize(DATA_PATH)!=0:\n",
    "        with open(DATA_PATH, \"r\") as f:\n",
    "            data= json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open(DATA_PATH, \"w\") as f:\n",
    "    for i in range(df.shape[0]):\n",
    "        #if df[\"ID\"][i] not in json_data:\n",
    "            data[df[\"ID\"][i]] = get_wiki_page(df[\"ID\"][i]).replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    #convert keys to string\n",
    "    data = {str(key):val for key, val in data.items()}\n",
    "    json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
